{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fine-Tuning using BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-xL-ZGnOPK_a"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD-FlLqW-ZnX"
      },
      "source": [
        "# **Sentiment Analysis : Fine-Tuning using BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2I3vpNJ-4kp"
      },
      "source": [
        "## **Teble of content**\n",
        "\n",
        "1. [Dataset Customization](#dataset-customization)\n",
        "2. [BERT Model](#bert-model)\n",
        "3. [Training Preparation](#training-preparation)\n",
        "4. [Training Loop](#training-loop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgxoGKC2LYhg"
      },
      "source": [
        "### Connect with `Google Drive`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyDODoaWC6KI",
        "outputId": "9ad09c1b-8d7a-4783-a8da-6fda05a50152"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW_-BzIfCRr9"
      },
      "source": [
        "### Install the `transformers` package\n",
        "> This package could be installed from [**Hugging Face**](https://huggingface.co/), it will gives us a PyTorch interface for working with BERT. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2y8SSwFL7aS",
        "outputId": "334ffb90-1b5c-41f6-e868-b00eee85531c"
      },
      "source": [
        "!pip install --upgrade transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91FedSxFMSsP"
      },
      "source": [
        "### Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW0Cs-w3MP9B"
      },
      "source": [
        "# Linear algebra\n",
        "import numpy as np\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHdCzlO-ZL9m"
      },
      "source": [
        "df = pd.read_csv('/gdrive/MyDrive/Movie Review.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "cLhpCovIZrlM",
        "outputId": "465e60f3-4a9d-4bc2-df88-41184602d90d"
      },
      "source": [
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "-JuzfVroaKtP",
        "outputId": "63ca9471-83cb-481a-9181-ff0a00b03634"
      },
      "source": [
        "df.groupby(['sentiment']).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           review\n",
              "sentiment        \n",
              "negative    25000\n",
              "positive    25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAYmw6TXAfbG"
      },
      "source": [
        "### **Dataset Cusomization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xL-ZGnOPK_a"
      },
      "source": [
        "> Here I'm going to transform the dataset into the format that our BERT model can be trained on. For this reason, I will:\n",
        "* use **`BertTokenizer`** to load the BERT pre-trained Tokenizer. As you you know before feeding our review to BERT, it *must be tokenized into tokens*, and then *these tokens must be mapped to their index in the tokenizer vocabulary*. The tokenization must be performed using the tokenizer integrated with BERT.\n",
        "* Format reviews by adding **special tokens** `[CLS]` to the start and `[SEP]` to the end  of each sentence, padding and truncatting sentence to single constant lenght since our reviews have varying lengths, and we will see explicitly how i defferentiate between `real tokens` and `padding tokens` with **attention mask**, just note that I will not truncate reviews this time!  here is the trick; padding is done with a special `[PAD]` token indexed with 0 in the BERT vocabulary.\n",
        "\n",
        "Please, note that BERT Model has two constraints:\n",
        "1. Sentences must be `padded` or `truncated` to a fixed length. In my case I padded reviews with a max length equals to 64 token\n",
        "2. Sentence maximum length is 512 tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi9ViEWtPShj"
      },
      "source": [
        "class MovieReviewDatSet:\n",
        "  def __init__(self, data_path):\n",
        "    self.data = pd.read_csv(data_path).fillna('none')\n",
        "    self.data.sentiment = self.data.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)\n",
        "    self.data = self.data.reset_index(drop=True)\n",
        "\n",
        "    # Load the BERT tokenizer using the uncased vesion\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "    self.max_len = 64\n",
        "    \n",
        "    self.review = self.data.review.values\n",
        "    self.label = self.data.sentiment.values\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.review)\n",
        "\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.review[item])\n",
        "    review = \" \".join(review.split())\n",
        "\n",
        "    '''\n",
        "    Apply the tokenizer to reviews, then\n",
        "    conevert tokens to ids based en BERT\n",
        "    vocabulary.\n",
        "    1. Splite the sentence into tokens\n",
        "    2. Add special tokens\n",
        "    3. Map tokens to their IDs\n",
        "    4. Padd reviews to the same length (64 token)\n",
        "    5. Create the attention mask which helps us diffrenciate\n",
        "      between real tokens from padded tokens marked with [PAD]\n",
        "      indexed with 0 in the vocabulary.\n",
        "    '''\n",
        "    inputs = self.tokenizer.encode_plus(\n",
        "        review,\n",
        "        None,\n",
        "        add_special_tokens = True, # Add special Token [SEP], [CLS]\n",
        "        max_length = self.max_len,\n",
        "        pad_to_max_length = True, # Pad all reviews\n",
        "    )\n",
        "\n",
        "    # Review tokens IDs\n",
        "    ids = inputs[\"input_ids\"]\n",
        "    mask = inputs[\"attention_mask\"]\n",
        "    token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "    # Convert every thing to tensors\n",
        "    samples = {\n",
        "        \"ids\" : torch.tensor(ids, dtype=torch.long),\n",
        "        \"mask\" : torch.tensor(mask, dtype=torch.long),\n",
        "        \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long),\n",
        "        \"labels\" : torch.tensor(self.label[item], dtype=torch.float)\n",
        "    }\n",
        "\n",
        "    return samples\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPMb5z3sPq8u"
      },
      "source": [
        "#### Split the Customized Dataset\n",
        "> Here, I'm loading and intantiating the dataset, and then customized it using `DataSetMovieReview` class.\n",
        "After I will split it into training data `90%` and validation data `10%`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZTWZDdGaoux",
        "outputId": "286b0db8-10ce-49c9-b7ad-8d959787b4a7"
      },
      "source": [
        "data_path = '/gdrive/MyDrive/Movie Review.csv'\n",
        "\n",
        "# Instantiate dataset\n",
        "dataset = MovieReviewDatSet(data_path)\n",
        "\n",
        "# Split data into train and valid subsets\n",
        "train_data, valid_data = train_test_split(dataset, test_size = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXNZMNUFcsOR",
        "outputId": "6598d069-d746-43f5-aef5-759aaf3e61a0"
      },
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ids': tensor([  101,  1000,  2054,  5650,  2179,  1000,  2001,  1037,  8242,  5456,\n",
            "         1012,  2004,  2517,  1998,  2856,  2011,  1037,  1012,  4670,  4330,\n",
            "         1010,  2023,  2003,  5257,  1997,  1037,  2346,  3185,  2007,  1037,\n",
            "        14046,  5649,  6925,  1010,  2004,  2092,  2004,  1037,  8774,  1997,\n",
            "         5456,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,\n",
            "         2065,  2017,  4033,  1005,  1056,  2464,  1996,  2143,  1010,  2672,\n",
            "         2017,  2323,  2644,   102]), 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(1.)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgBSASLsPGV_"
      },
      "source": [
        "* `Attention Mask` is just an array of **1s** and **0s** indicating which tokens are padded and which are not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGhkvMwXWshI"
      },
      "source": [
        "#### DataLoader\n",
        "Here, I'm creating DataLoaders for training and validation sets. However, the DataLoader needs to know our `batch size` for training and validation, for this reason:\n",
        "* I specified it and setting equal to 8 for training and 4 for validation\n",
        "* I specified `num_workers` for training and validation 4 and 1 respectively. It basically indicates how many subprocesses to use for data loading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0Ov-AmAbZfo"
      },
      "source": [
        "# Train data\n",
        "train_dataloader = DataLoader(train_data,\n",
        "                              batch_size = 8,\n",
        "                              num_workers = 4)\n",
        "\n",
        "# Valid data\n",
        "valid_dataloader = DataLoader(valid_data,\n",
        "                              batch_size = 4,\n",
        "                              num_workers = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RDwJV-Fh1MB",
        "outputId": "f23ecd97-a17c-439f-cec3-fe61850f9c40"
      },
      "source": [
        "type(train_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p68dmB8qd4Hc"
      },
      "source": [
        "### **BERT Model**\n",
        "Since our dataset is ready and well costumized, it's time to fine tune BERT Model. SO, we need to adapte pre-trained BertModel by modifying its output for our clasification task, then train it on our dataset.\n",
        "As might know there are a lot of classes that we can use to fine-tune BERT like `BertModel`, `BertForPreTraining`, `BertForMaskedLM` and list is too long. In our case, we will be using **`BertModel`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78ZGJsH9dyLu"
      },
      "source": [
        "class BERTModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BERTModel, self).__init__()\n",
        "\n",
        "    # Load pre-trained model (weights)\n",
        "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "    self.bert_drop = nn.Dropout(0.3)\n",
        "\n",
        "    # Linear classifier layer on top\n",
        "    self.out = nn.Linear(768, 1)\n",
        "\n",
        "  # def forward(self, ids, mask, token_type_ids):\n",
        "  #   _, o2 = self.bert(ids, attention_mask=mask, token_type_ids = token_type_ids)\n",
        "  #   bo = self.bert_drop(o2)\n",
        "  #   output = self.out(bo)\n",
        "  \n",
        "  def forward(self, ids, mask):\n",
        "    o2 = self.bert(ids, attention_mask=mask)\n",
        "    pooled_ouput = o2['pooler_output']    \n",
        "    bo = self.bert_drop(pooled_ouput)\n",
        "    output = self.out(bo)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD0MyZb7nI5q",
        "outputId": "0614af6f-fc38-406c-f667-048792d25312"
      },
      "source": [
        "# Get GPU device name; PyTorch will use this GPU \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = BERTModel()\n",
        "model.to(device) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTModel(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (bert_drop): Dropout(p=0.3, inplace=False)\n",
              "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY3QlFiEhDFE"
      },
      "source": [
        ">If you took a close look at the model's parameters using the summary above.you will be able to see the:\n",
        "1. **`Embedding layer`** including `token embedding`, `segment embedding`, and `positional embedding`.\n",
        "1. First **`12 transformers`** (0->11)\n",
        "1. **`Output layer`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1ZcW342-WRR"
      },
      "source": [
        "### **Optimizer & Scheduler**\n",
        "> Our Model is loaded and modified successfully. So, we have now to chose the `optimizer` and `parameterize` our scheduler.\n",
        "If you come back to the [original paper](https://arxiv.org/pdf/1810.04805.pdf), the authors recommended using the following values: \n",
        "* **`Batch Size`**: 16, 32\n",
        "* **`Learning Rate`**: 5e-5, 3e-5, 2e-5\n",
        "* **`Number of epochs`**: 2, 3, 4\n",
        "\n",
        "In our case, I chose:\n",
        "- **Batch size** equals to 8 set when creating the DataLoader.\n",
        "- `AdamW` optimizer which is a class from the hugging face library with **Lr** : 2e-5 as shxon in script below.\n",
        "- **Epochs** equal to 4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOw1S3xF-Vvq"
      },
      "source": [
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  correct_bias = False) # We chose to correct bias\n",
        "\n",
        "# Total number of training steps is number of batchs\n",
        "total_steps = len(train_dataloader)\n",
        "\n",
        "# create the learning rate Scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = 0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAXNvRlx-REV"
      },
      "source": [
        "### **Training Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U70dkJc9-WUm"
      },
      "source": [
        "> Here, I will define some helper function like:\n",
        "1. **`loss_fn`** fo calculating the loss function.\n",
        "2. **`train_fn`** for the training phase where I will load the data onto the GPU, feed it through the network, perform the backward pass, and update the parameters with optimizer and the Learning rate.\n",
        "3. **`eval_fn`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4s6QHpEBRpC"
      },
      "source": [
        "def loss_fn(outputs, labels):\n",
        "  return nn.BCEWithLogitsLoss()(outputs, labels.view(-1, 1))\n",
        "\n",
        "def train_fn(train_dataloader, model, optimizer, device, scheduler):\n",
        "  model.train()\n",
        "\n",
        "  for bi, d in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "    ids = d[\"ids\"]\n",
        "    # token_type_ids = d[\"token_type_ids\"]\n",
        "    mask = d[\"mask\"]\n",
        "    labels = d[\"labels\"]\n",
        "\n",
        "    # Load data onto the GPU\n",
        "    ids = ids.to(device, dtype=torch.long)\n",
        "   # token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "    mask = mask.to(device, dtype = torch.long)\n",
        "    labels = labels.to(device, dtype = torch.float)\n",
        "\n",
        "    # Clear out the gradients of the previous pass\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward & backward passes\n",
        "    outputs = model(ids=ids,\n",
        "                   # token_type_ids = token_type_ids,\n",
        "                    mask=mask)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the parameters \n",
        "    optimizer.step()\n",
        "\n",
        "    # Track varaibles for monitoring progress\n",
        "    # Update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "def eval_fn(train_dataloader, model, device):\n",
        "  model.eval()\n",
        "  fin_labels = []\n",
        "  fin_outputs = []\n",
        "  with torch.no_grad():\n",
        "    for bi, d in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "      ids = d[\"ids\"]\n",
        "      #token_type_ids = d[\"token_type_ids\"]\n",
        "      mask = d[\"mask\"]\n",
        "      labels = d[\"labels\"]\n",
        "\n",
        "      # Load the data onto the GPU\n",
        "      ids = ids.to(device, dtype=torch.long)\n",
        "      #token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "      mask = mask.to(device, dtype=torch.float)\n",
        "      labels = labels.to(device, dtype=torch.float)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(ids=ids, \n",
        "                      #token_type_ids=token_type_ids, \n",
        "                      mask=mask)\n",
        "      fin_labels.extend(labels.cpu().detach().numpy().tolist())\n",
        "      fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "  return fin_outputs, fin_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Herf-_JgFG"
      },
      "source": [
        "### **Training loop**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viUp9238EAEc"
      },
      "source": [
        "> Finally, we come to the final phase, where I will fundamentally for each pass in our loop have a **`training`** and **`validation`** phase using our three helper functions defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQaLNbXpEXdl",
        "outputId": "9837ef24-061f-4746-c9e6-4ec2688b9d81"
      },
      "source": [
        "EPOCHS = 4\n",
        "best_accuracy = 0\n",
        "for epoch in range(EPOCHS):\n",
        "  train_fn(train_dataloader, model, optimizer, device, scheduler)\n",
        "  outputs, labels = eval_fn(valid_dataloader, model, device)\n",
        "  outputs = np.array(outputs) >= 0.5\n",
        "\n",
        "  # Calculate the accuracy for each pass\n",
        "  accuracy = metrics.accuracy_score(labels, outputs)\n",
        "  print(f\"The Accuracy Score is = {accuracy}\")\n",
        "  \n",
        "  # Save our best model having the best accuracy\n",
        "  if accuracy > best_accuracy:\n",
        "    torch.save(model.state_dict(), \"/gdrive/MyDrive/BERTModel.bin\")\n",
        "    best_accuracy = accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5625/5625 [06:51<00:00, 13.68it/s]\n",
            "100%|██████████| 1250/1250 [00:16<00:00, 76.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Accuracy Score is = 0.8356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5625/5625 [06:51<00:00, 13.66it/s]\n",
            "100%|██████████| 1250/1250 [00:16<00:00, 76.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Accuracy Score is = 0.8356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 5625/5625 [06:51<00:00, 13.67it/s]\n",
            "100%|██████████| 1250/1250 [00:16<00:00, 76.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Accuracy Score is = 0.8356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 5625/5625 [06:52<00:00, 13.65it/s]\n",
            "100%|██████████| 1250/1250 [00:16<00:00, 76.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Accuracy Score is = 0.8356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aP0lqRH4GFg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}